{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADAGRAD OPTIMIZER IMPLEMENTATION.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMgySyk6/g9i+L3d3YjKm1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/ADAGRAD-OPTIMIZER-IMPLEMENTATION-FROM-SCRATCH/blob/master/ADAGRAD_OPTIMIZER_IMPLEMENTATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkduTRtAE-J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Z7mjTPFYdN",
        "colab_type": "text"
      },
      "source": [
        "$$\\begin{align}\n",
        "\\mathbf{w}_{t+1} &amp;= \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}\\\\\n",
        "&amp;= \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t})}{\\partial \\mathbf{w}_{t}}\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmzdModIGH5V",
        "colab_type": "text"
      },
      "source": [
        "$G_{t}$: the sum of the squares of the past gradients w.r.t. to all parameters $\\mathbf{w}$ along its diagonal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocMYiFcLGK4p",
        "colab_type": "text"
      },
      "source": [
        "where $G_{t} \\in \\mathbb{R}^{d \\times d}$ here is a diagonal matrix "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nisS0EfpGTkn",
        "colab_type": "text"
      },
      "source": [
        "where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\\mathbf{w}_{i}$ up to time step $t$,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su_Rdl18GfgB",
        "colab_type": "text"
      },
      "source": [
        "while $\\epsilon$ is a smoothing term that avoids division by zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcLL_MWGzOL",
        "colab_type": "text"
      },
      "source": [
        "# implementation of the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFmOdm7YHTfR",
        "colab_type": "text"
      },
      "source": [
        "importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKzr1a8pFukP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import LogNorm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psf1wakqHs3e",
        "colab_type": "text"
      },
      "source": [
        "Beale function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2QXzNW6HvVf",
        "colab_type": "text"
      },
      "source": [
        "$$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UCizbr8H8Nl",
        "colab_type": "text"
      },
      "source": [
        "analytic solution (global minima)\n",
        "$(x, y) = (3, 0.5)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHfUOgb8Hcds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oCjqC-CIKjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradients(x, y):\n",
        "  \"\"\"Gradient of Beale function.\n",
        "\n",
        "  Args:\n",
        "    x: x-dimension of inputs\n",
        "    y: y-dimension of inputs\n",
        "\n",
        "  Returns:\n",
        "    grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "      dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "      dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "  \"\"\"\n",
        "  dx = 2. * ( (1.5 - x + x * y) * (y - 1) + \\\n",
        "                (2.25 - x + x * y**2) * (y**2 - 1) + \\\n",
        "                (2.625 - x + x * y**3) * (y**3 - 1) )\n",
        "  dy = 2. * ( (1.5 - x + x * y) * x + \\\n",
        "              (2.25 - x + x * y**2) * 2. * x * y + \\\n",
        "              (2.625 - x + x * y**3) * 3. * x * y**2 )\n",
        "  grads = np.array([dx, dy])\n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVnJhKFGIoi1",
        "colab_type": "text"
      },
      "source": [
        "Build a Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbhyhF4FISUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdagradOptimizer():\n",
        "  def __init__(self, function, gradients, x_init=None, y_init=None, learning_rate=0.01, initial_accumulator_value=0.1):\n",
        "    self.f = function\n",
        "    self.g = gradients\n",
        "    scale = 3.0\n",
        "    self.vars = np.zeros([2])\n",
        "\n",
        "    if x_init is not None:\n",
        "      self.vars[0] = x_init\n",
        "    else:\n",
        "      self.vars[0] = np.random.uniform(low=-scale, high=scale)\n",
        "\n",
        "    if y_init is not None:\n",
        "      self.vars[1] = y_init\n",
        "    else:\n",
        "      self.vars[1] = np.random.uniform(low=-scale, high=scale)\n",
        "    \n",
        "    print(\"x_init: {:.3f}\".format(self.vars[0]))\n",
        "    print(\"y_init: {:.3f}\".format(self.vars[1]))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.grads_squared = np.zeros([2])\n",
        "    self.grads_squared.fill(initial_accumulator_value)\n",
        "    self.epsilon = 1e-7\n",
        "\n",
        "    # for accumulation of loss and path (w, b)\n",
        "    self.z_history = []\n",
        "    self.x_history = []\n",
        "    self.y_history = []\n",
        "\n",
        "  def func(self, variables):\n",
        "    \"\"\"Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      z: Beale function value at (x, y)\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    z = self.f(x, y)\n",
        "    return z\n",
        "  def gradients(self, variables):\n",
        "    \"\"\"Gradient of Beale function.\n",
        "    \n",
        "    Args:\n",
        "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
        "        x: x-dimension of inputs\n",
        "        y: y-dimension of inputs\n",
        "      \n",
        "    Returns:\n",
        "      grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
        "        dx: gradient of Beale function with respect to x-dimension of inputs\n",
        "        dy: gradient of Beale function with respect to y-dimension of inputs\n",
        "    \"\"\"\n",
        "    x, y = variables\n",
        "    grads = self.g(x, y)\n",
        "    return grads\n",
        "  \n",
        "  def weights_update(self, grads):\n",
        "    \"\"\"Weights update using adagrad.\n",
        "    grads2 = grads2 + grads**2\n",
        "    w' = w - lr * grads / (sqrt(grads2) + epsilon)\"\"\"\n",
        "    self.grads_squared = self.grads_squared + grads**2\n",
        "    self.vars = self.vars - self.lr * grads / (np.sqrt(self.grads_squared) + self.epsilon)\n",
        "\n",
        "  def history_update(self, z, x, y):\n",
        "    \"\"\"Accumulate all interesting variables\n",
        "    \"\"\"\n",
        "    self.z_history.append(z)\n",
        "    self.x_history.append(x)\n",
        "    self.y_history.append(y)\n",
        "\n",
        "  def train(self, max_steps):\n",
        "    pre_z = 0.0\n",
        "    print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(0, self.func(self.vars), self.x, self.y))\n",
        "\n",
        "    file = open('adagrad.txt', 'w')\n",
        "    file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "\n",
        "    for step in range(max_steps):  \n",
        "      self.z = self.func(self.vars)\n",
        "      self.history_update(self.z, self.x, self.y)\n",
        "\n",
        "      self.grads = self.gradients(self.vars)\n",
        "      self.weights_update(self.grads)\n",
        "      file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
        "      \n",
        "      if (step+1) % 100 == 0:\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}  dx: {:.5f}  dy: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y, self.dx, self.dy))\n",
        "\n",
        "      if np.abs(pre_z - self.z) < 1e-7:\n",
        "        print(\"Enough convergence\")\n",
        "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y))\n",
        "        self.z = self.func(self.vars)\n",
        "        self.history_update(self.z, self.x, self.y)\n",
        "        break\n",
        "      \n",
        "      pre_z = self.z\n",
        "    file.close()\n",
        "  \n",
        "    self.x_history = np.array(self.x_history)\n",
        "    self.y_history = np.array(self.y_history)\n",
        "    self.path = np.concatenate((np.expand_dims(self.x_history, 1), np.expand_dims(self.y_history, 1)), axis=1).T\n",
        "  \n",
        "  @property\n",
        "  def x(self):\n",
        "    return self.vars[0]\n",
        "  \n",
        "  @property\n",
        "  def y(self):\n",
        "    return self.vars[1]\n",
        "  \n",
        "  @property\n",
        "  def dx(self):\n",
        "    return self.grads[0]\n",
        "  \n",
        "  @property\n",
        "  def dy(self):\n",
        "    return self.grads[1]\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M8jiWXdUUSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}